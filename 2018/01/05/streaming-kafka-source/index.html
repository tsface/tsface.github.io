<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('http://yoursite.com').hostname,
    root: '/',
    scheme: 'Mist',
    version: '7.6.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta name="description" content="KafkaSourceProvider自定义SourceProvider可以向Structured Streaming注册自定义Source，其实现了StreamSourceProvider和 DataSourceRegister接口。需要重写以下三个方法：  sourceSchema获取数据源的schema信息，返回类型是Tuple[_,_],其中的key代表数据源的别名，value表示数据源的">
<meta name="keywords" content="bigdata,Structured Streaming,Spark,kafka">
<meta property="og:type" content="article">
<meta property="og:title" content="Structured Streaming Kafka Source 源码分析">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2018&#x2F;01&#x2F;05&#x2F;streaming-kafka-source&#x2F;index.html">
<meta property="og:site_name" content="LIYAN BLOG">
<meta property="og:description" content="KafkaSourceProvider自定义SourceProvider可以向Structured Streaming注册自定义Source，其实现了StreamSourceProvider和 DataSourceRegister接口。需要重写以下三个方法：  sourceSchema获取数据源的schema信息，返回类型是Tuple[_,_],其中的key代表数据源的别名，value表示数据源的">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2019-12-06T04:51:11.144Z">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/2018/01/05/streaming-kafka-source/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>Structured Streaming Kafka Source 源码分析 | LIYAN BLOG</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">LIYAN BLOG</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">生活,家庭,计算机</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/05/streaming-kafka-source/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/myphoto.jpg">
      <meta itemprop="name" content="liyan">
      <meta itemprop="description" content="我的世界不止计算机">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LIYAN BLOG">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Structured Streaming Kafka Source 源码分析
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-01-05 00:00:00" itemprop="dateCreated datePublished" datetime="2018-01-05T00:00:00+08:00">2018-01-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-12-06 12:51:11" itemprop="dateModified" datetime="2019-12-06T12:51:11+08:00">2019-12-06</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="KafkaSourceProvider"><a href="#KafkaSourceProvider" class="headerlink" title="KafkaSourceProvider"></a>KafkaSourceProvider</h2><p>自定义<code>SourceProvider</code>可以向<code>Structured Streaming</code>注册自定义Source，其实现了<code>StreamSourceProvider</code>和 <code>DataSourceRegister</code>接口。需要重写以下三个方法：</p>
<ul>
<li>sourceSchema<br>获取数据源的schema信息，返回类型是<code>Tuple[_,_]</code>,其中的key代表数据源的别名，value表示数据源的schema</li>
<li>createSource<br>创建一个<code>org.apache.spark.sql.execution.streaming.Source</code>接口的实现，表示数据源对象</li>
<li>shortName<br>自定义的数据源名称，这个数据源名称会提供给用户的format方法设置数据源</li>
</ul>
<a id="more"></a>


<h2 id="KafkaSourceRDD"><a href="#KafkaSourceRDD" class="headerlink" title="KafkaSourceRDD"></a>KafkaSourceRDD</h2><ul>
<li><p>KafkaSourceRDDOffsetRange：KafkaSourceRDD，其内部一个Partition的Offset范围</p>
</li>
<li><p>KafkaSourceRDDPartition：数据结构KafkaSourceRDD数据结构定义的分区，其是<code>org.apache.spark.Partition</code>的子类</p>
</li>
<li><p>KafkaSourceRDD: kafka自定义实现的一个RDD，RDD的数据来源于kafka</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KafkaSourceRDD</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">                      sc: <span class="type">SparkContext</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                      executorKafkaParams: ju.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>],</span></span></span><br><span class="line"><span class="class"><span class="params">                      offsetRanges: <span class="type">Seq</span>[<span class="type">KafkaSourceRDDOffsetRange</span>],</span></span></span><br><span class="line"><span class="class"><span class="params">                      pollTimeoutMs: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                      failOnDataLoss: <span class="type">Boolean</span></span>)</span></span><br></pre></td></tr></table></figure>
<p>构造器的代码可以看出，其需要的关键参数是<code>executorKafkaParams</code>,<code>offsetRanges</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(</span><br><span class="line">      thePart: <span class="type">Partition</span>,</span><br><span class="line">      context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">ConsumerRecord</span>[<span class="type">Array</span>[<span class="type">Byte</span>], <span class="type">Array</span>[<span class="type">Byte</span>]]] = &#123;</span><br><span class="line">    <span class="keyword">val</span> range = thePart.asInstanceOf[<span class="type">KafkaSourceRDDPartition</span>].offsetRange</span><br><span class="line">    assert(</span><br><span class="line">      range.fromOffset &lt;= range.untilOffset,</span><br><span class="line">      <span class="string">s"Beginning offset <span class="subst">$&#123;range.fromOffset&#125;</span> is after the ending offset <span class="subst">$&#123;range.untilOffset&#125;</span> "</span> +</span><br><span class="line">        <span class="string">s"for topic <span class="subst">$&#123;range.topic&#125;</span> partition <span class="subst">$&#123;range.partition&#125;</span>. "</span> +</span><br><span class="line">        <span class="string">"You either provided an invalid fromOffset, or the Kafka topic has been damaged"</span>)</span><br><span class="line">    <span class="comment">//如果读取的fromOffset和untilOffset一致，则返回一个空的Iterator</span></span><br><span class="line">    <span class="keyword">if</span> (range.fromOffset == range.untilOffset) &#123;</span><br><span class="line">      logInfo(<span class="string">s"Beginning offset <span class="subst">$&#123;range.fromOffset&#125;</span> is the same as ending offset "</span> +</span><br><span class="line">        <span class="string">s"skipping <span class="subst">$&#123;range.topic&#125;</span> <span class="subst">$&#123;range.partition&#125;</span>"</span>)</span><br><span class="line">      <span class="type">Iterator</span>.empty</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">NextIterator</span>[<span class="type">ConsumerRecord</span>[<span class="type">Array</span>[<span class="type">Byte</span>], <span class="type">Array</span>[<span class="type">Byte</span>]]]() &#123;</span><br><span class="line">        <span class="keyword">val</span> consumer = <span class="type">CachedKafkaConsumer</span>.getOrCreate(</span><br><span class="line">          range.topic, range.partition, executorKafkaParams)</span><br><span class="line">        <span class="keyword">var</span> requestOffset = range.fromOffset</span><br><span class="line"></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getNext</span></span>(): <span class="type">ConsumerRecord</span>[<span class="type">Array</span>[<span class="type">Byte</span>], <span class="type">Array</span>[<span class="type">Byte</span>]] = &#123;</span><br><span class="line">          <span class="keyword">if</span> (requestOffset &gt;= range.untilOffset) &#123;</span><br><span class="line">            <span class="comment">// Processed all offsets in this partition.</span></span><br><span class="line">            finished = <span class="literal">true</span></span><br><span class="line">            <span class="literal">null</span></span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          	<span class="comment">//通过consumer获取record</span></span><br><span class="line">            <span class="keyword">val</span> r = consumer.get(requestOffset, range.untilOffset, pollTimeoutMs, failOnDataLoss)</span><br><span class="line">            <span class="comment">//如果读取的数据为空，则说明kafka上这条数据已经没有了，则读取失败</span></span><br><span class="line">            <span class="keyword">if</span> (r == <span class="literal">null</span>) &#123;</span><br><span class="line">              <span class="comment">// Losing some data. Skip the rest offsets in this partition.</span></span><br><span class="line">              finished = <span class="literal">true</span></span><br><span class="line">              <span class="literal">null</span></span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">              <span class="comment">//当前的offset +1 作为读取下条数据的offset</span></span><br><span class="line">              requestOffset = r.offset + <span class="number">1</span></span><br><span class="line">              r</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">//......</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h2 id="kakfa消费者管理对象CachedKafkaConsumer"><a href="#kakfa消费者管理对象CachedKafkaConsumer" class="headerlink" title="kakfa消费者管理对象CachedKafkaConsumer"></a>kakfa消费者管理对象CachedKafkaConsumer</h2><ul>
<li>通过 groupId 获取一个kafka Consumer, 指定topic和Partition信息，如果没有找到则按照kafka的信息创建一个Consumer<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getOrCreate</span></span>(</span><br><span class="line">                 topic: <span class="type">String</span>,</span><br><span class="line">                 partition: <span class="type">Int</span>,</span><br><span class="line">                 kafkaParams: ju.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>]): <span class="type">CachedKafkaConsumer</span> = synchronized &#123;</span><br><span class="line">  <span class="keyword">val</span> groupId = kafkaParams.get(<span class="type">ConsumerConfig</span>.<span class="type">GROUP_ID_CONFIG</span>).asInstanceOf[<span class="type">String</span>]</span><br><span class="line">  <span class="keyword">val</span> topicPartition = <span class="keyword">new</span> <span class="type">TopicPartition</span>(topic, partition)</span><br><span class="line">  <span class="keyword">val</span> key = <span class="type">CacheKey</span>(groupId, topicPartition)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// If this is reattempt at running the task, then invalidate cache and start with</span></span><br><span class="line">  <span class="comment">// a new consumer</span></span><br><span class="line">  <span class="keyword">if</span> (<span class="type">TaskContext</span>.get != <span class="literal">null</span> &amp;&amp; <span class="type">TaskContext</span>.get.attemptNumber &gt; <span class="number">1</span>) &#123;</span><br><span class="line">    <span class="keyword">val</span> removedConsumer = cache.remove(key)</span><br><span class="line">    <span class="keyword">if</span> (removedConsumer != <span class="literal">null</span>) &#123;</span><br><span class="line">      removedConsumer.close()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">CachedKafkaConsumer</span>(topicPartition, kafkaParams)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (!cache.containsKey(key)) &#123;</span><br><span class="line">      cache.put(key, <span class="keyword">new</span> <span class="type">CachedDataCacheConsumer</span>(topicPartition, kafkaParams))</span><br><span class="line">    &#125;</span><br><span class="line">    cache.get(key)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>CachedKafkaConsumer对外开放的数据读取接口<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get</span></span>(</span><br><span class="line">         offset: <span class="type">Long</span>,</span><br><span class="line">         untilOffset: <span class="type">Long</span>,</span><br><span class="line">         pollTimeoutMs: <span class="type">Long</span>,</span><br><span class="line">         failOnDataLoss: <span class="type">Boolean</span>): <span class="type">ConsumerRecord</span>[<span class="type">Array</span>[<span class="type">Byte</span>], <span class="type">Array</span>[<span class="type">Byte</span>]]</span><br></pre></td></tr></table></figure></li>
<li>从kafka读取数据<ul>
<li>如果是第一次读取，或者每次获取新的迭代容器，则首先调用seek，然后调用poll</li>
<li>如果是从迭代器中获取数据，一条一条的取出数据<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">fetchData</span></span>( offset: <span class="type">Long</span>,untilOffset: <span class="type">Long</span>,pollTimeoutMs: <span class="type">Long</span>,</span><br><span class="line">                         failOnDataLoss: <span class="type">Boolean</span>): <span class="type">ConsumerRecord</span>[<span class="type">Array</span>[<span class="type">Byte</span>], <span class="type">Array</span>[<span class="type">Byte</span>]] = &#123;</span><br><span class="line">    <span class="keyword">if</span> (offset != nextOffsetInFetchedData || !fetchedData.hasNext()) &#123;</span><br><span class="line">      <span class="comment">// This is the first fetch, or the last pre-fetched data has been drained.</span></span><br><span class="line">      <span class="comment">// Seek to the offset because we may call seekToBeginning or seekToEnd before this.</span></span><br><span class="line">      seek(offset)</span><br><span class="line">      poll(pollTimeoutMs)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!fetchedData.hasNext()) &#123;</span><br><span class="line">      <span class="comment">// We cannot fetch anything after `poll`. Two possible cases:</span></span><br><span class="line">      <span class="comment">// - `offset` is out of range so that Kafka returns nothing. Just throw</span></span><br><span class="line">      <span class="comment">// `OffsetOutOfRangeException` to let the caller handle it.</span></span><br><span class="line">      <span class="comment">// - Cannot fetch any data before timeout. TimeoutException will be thrown.</span></span><br><span class="line">      <span class="keyword">val</span> (earliestOffset, latestOffset) = getAvailableOffsetRange()</span><br><span class="line">      <span class="keyword">if</span> (offset &lt; earliestOffset || offset &gt;= latestOffset) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">OffsetOutOfRangeException</span>(</span><br><span class="line">          <span class="type">Map</span>(topicPartition -&gt; java.lang.<span class="type">Long</span>.valueOf(offset)).asJava)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">TimeoutException</span>(</span><br><span class="line">          <span class="string">s"Cannot fetch record for offset <span class="subst">$offset</span> in <span class="subst">$pollTimeoutMs</span> milliseconds"</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> record = fetchedData.next()</span><br><span class="line">      nextOffsetInFetchedData = record.offset + <span class="number">1</span></span><br><span class="line">      <span class="comment">// In general, Kafka uses the specified offset as the start point, and tries to fetch the next</span></span><br><span class="line">      <span class="comment">// available offset. Hence we need to handle offset mismatch.</span></span><br><span class="line">      <span class="keyword">if</span> (record.offset &gt; offset) &#123;</span><br><span class="line">        <span class="comment">// This may happen when some records aged out but their offsets already got verified</span></span><br><span class="line">        <span class="keyword">if</span> (failOnDataLoss) &#123;</span><br><span class="line">          reportDataLoss(<span class="literal">true</span>, <span class="string">s"Cannot fetch records in [<span class="subst">$offset</span>, <span class="subst">$&#123;record.offset&#125;</span>)"</span>)</span><br><span class="line">          <span class="comment">// Never happen as "reportDataLoss" will throw an exception</span></span><br><span class="line">          <span class="literal">null</span></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="keyword">if</span> (record.offset &gt;= untilOffset) &#123;</span><br><span class="line">            reportDataLoss(<span class="literal">false</span>, <span class="string">s"Skip missing records in [<span class="subst">$offset</span>, <span class="subst">$untilOffset</span>)"</span>)</span><br><span class="line">            <span class="literal">null</span></span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            reportDataLoss(<span class="literal">false</span>, <span class="string">s"Skip missing records in [<span class="subst">$offset</span>, <span class="subst">$&#123;record.offset&#125;</span>)"</span>)</span><br><span class="line">            record</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (record.offset &lt; offset) &#123;</span><br><span class="line">        <span class="comment">// This should not happen. If it does happen, then we probably misunderstand Kafka internal</span></span><br><span class="line">        <span class="comment">// mechanism.</span></span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(</span><br><span class="line">          <span class="string">s"Tried to fetch <span class="subst">$offset</span> but the returned record offset was <span class="subst">$&#123;record.offset&#125;</span>"</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        record</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/bigdata/" rel="tag"># bigdata</a>
              <a href="/tags/Structured-Streaming/" rel="tag"># Structured Streaming</a>
              <a href="/tags/Spark/" rel="tag"># Spark</a>
              <a href="/tags/kafka/" rel="tag"># kafka</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2018/01/05/scala-implicit/" rel="prev" title="Scala-implicit">
      <i class="fa fa-chevron-left"></i> Scala-implicit
    </a></div>
      <div class="post-nav-item">
    <a href="/2018/01/05/LogSegment/" rel="next" title="Kafka LogSegment 源码分析">
      Kafka LogSegment 源码分析 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#KafkaSourceProvider"><span class="nav-number">1.</span> <span class="nav-text">KafkaSourceProvider</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#KafkaSourceRDD"><span class="nav-number">2.</span> <span class="nav-text">KafkaSourceRDD</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kakfa消费者管理对象CachedKafkaConsumer"><span class="nav-number">3.</span> <span class="nav-text">kakfa消费者管理对象CachedKafkaConsumer</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="liyan"
      src="/images/myphoto.jpg">
  <p class="site-author-name" itemprop="name">liyan</p>
  <div class="site-description" itemprop="description">我的世界不止计算机</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">17</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">32</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/tsface" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;tsface" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/mailto:hanily@msn.com" title="E-Mail → mailto:hanily@msn.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">liyan</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.0.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> v7.6.0
  </div>

        








        
      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/muse.js"></script>
<script src="/js/next-boot.js"></script>



  
















  

  

</body>
</html>
